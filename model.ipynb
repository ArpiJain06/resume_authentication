{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import keras\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score, log_loss, cohen_kappa_score, roc_auc_score\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import MobileNetV2\n",
        "from keras.optimizers import Adam\n",
        "from keras import Input\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "wsv33dss2b74"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_folder_for_training = '/content/drive/MyDrive/datasets used for part 10/resumes'\n",
        "non_resume_folder_for_training = '/content/drive/MyDrive/datasets used for part 10/not_resumes'\n",
        "resume_folder_for_testing ='/content/drive/MyDrive/datasets used for part 10/resumes for testing'\n",
        "non_resume_folder_for_testing = '/content/drive/MyDrive/datasets used for part 10/not_resumes for testing'"
      ],
      "metadata": {
        "id": "0GMplsFN50zq"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Explore the Data\n",
        "def load_and_preprocess_data(data_dir, label, augmentation):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(data_dir):\n",
        "        img_path = os.path.join(data_dir, filename)\n",
        "\n",
        "        # Check if file exists and is a valid image\n",
        "        if os.path.isfile(img_path):\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            # Check if image is successfully loaded\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, (128, 128))\n",
        "                img = img / 255.0\n",
        "                data.append(img)\n",
        "                labels.append(label)\n",
        "\n",
        "    data = np.array(data)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    if augmentation:\n",
        "        datagen = ImageDataGenerator(\n",
        "            rotation_range=20,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            zoom_range=0.1,\n",
        "            shear_range=0.2,\n",
        "            horizontal_flip=True\n",
        "        )\n",
        "        datagen.fit(data)\n",
        "        augmented_data = datagen.flow(data, labels, batch_size=len(data), shuffle=False).next()\n",
        "        data, labels = augmented_data[0], labels\n",
        "\n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "uFlq-ztV2cA1"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data from the two folders\n",
        "resume_data_aug, resume_labels_aug = load_and_preprocess_data(resume_folder_for_training , 1, True)\n",
        "non_resume_data_aug, non_resume_labels_aug = load_and_preprocess_data(non_resume_folder_for_training , 0, True)\n",
        "\n",
        "resume_data, resume_labels = load_and_preprocess_data(resume_folder_for_training , 1, False)\n",
        "non_resume_data, non_resume_labels = load_and_preprocess_data(non_resume_folder_for_training , 0, False)\n",
        "\n",
        "resume_data_for_testing,  resume_data_for_testing_labels = load_and_preprocess_data(resume_folder_for_testing , 1, False)\n",
        "non_resume_data_for_testing,  non_resume_data_for_testing_labels = load_and_preprocess_data(non_resume_folder_for_testing , 0, False)"
      ],
      "metadata": {
        "id": "VpOcE4wr2cEp"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine data and labels\n",
        "X_train_aug = np.concatenate((resume_data_aug, non_resume_data_aug), axis=0)\n",
        "y_train_aug = np.concatenate((resume_labels_aug, non_resume_labels_aug), axis=0)\n",
        "\n",
        "X_train = np.concatenate((resume_data, non_resume_data), axis=0)\n",
        "y_train = np.concatenate((resume_labels, non_resume_labels), axis=0)\n",
        "\n",
        "X_test = np.concatenate((resume_data_for_testing, non_resume_data_for_testing), axis=0)\n",
        "y_test = np.concatenate((resume_labels, non_resume_labels), axis=0)"
      ],
      "metadata": {
        "id": "DFk9BUKm5YU_"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the data\n",
        "augmented_data = list(zip(X_train_aug, y_train_aug))\n",
        "np.random.shuffle(augmented_data)\n",
        "X_train_aug, y_train_aug = zip(*augmented_data)\n",
        "X_train_aug = np.array(X_train_aug)\n",
        "y_train_aug = np.array(y_train_aug)"
      ],
      "metadata": {
        "id": "1RGwPvZe5Y04"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'y_train' is your training labels\n",
        "class_distribution = pd.Series(y_train.flatten()).value_counts()\n",
        "print(\"Class Distribution:\")\n",
        "print(class_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdxXDmhG62It",
        "outputId": "0c296ff3-3fda-40f0-d488-19cff7578048"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution:\n",
            "1    90\n",
            "0    90\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(\n",
        "    X_train_aug, y_train_aug, test_size=0.15, random_state=42 , stratify=y_train_aug\n",
        ")"
      ],
      "metadata": {
        "id": "E8xzxKsagmXN"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ImageDataGenerator for training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Define ImageDataGenerator for testing data\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Flow training images in batches of 32 using train_datagen generator\n",
        "train_generator = train_datagen.flow(\n",
        "    X_train_split,\n",
        "    y_train_split,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Flow testing images in batches of 32 using test_datagen generator\n",
        "test_generator = test_datagen.flow(\n",
        "    X_test_split,\n",
        "    y_test_split,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "UXg8uLi4Y-3J"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-trained MobileNetV2 model with weights from 'imagenet'\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=Input(shape=(128, 128, 3)))\n",
        "\n",
        "# Freeze the convolutional layers to retain pre-trained features\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification layers\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "dropout = 0.4\n",
        "x = Dense(1, activation='sigmoid')(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1jaoSxiXJwK",
        "outputId": "61bc472c-81cc-4263-925c-f01312b4da81"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N661gKBrXJz8",
        "outputId": "3c2b95b9-b35f-4e5c-e1f4-13baeee999c3"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 10\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=test_generator\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4AR24VXXJ2M",
        "outputId": "1ed4ae1a-8a55-4c1e-c920-4683a13a380c"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5/5 [==============================] - 8s 1s/step - loss: 0.7016 - accuracy: 0.4706 - val_loss: 0.6984 - val_accuracy: 0.4815\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 2s 444ms/step - loss: 0.6935 - accuracy: 0.5098 - val_loss: 0.6898 - val_accuracy: 0.5185\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 3s 464ms/step - loss: 0.6944 - accuracy: 0.4902 - val_loss: 0.6894 - val_accuracy: 0.7778\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 5s 938ms/step - loss: 0.6913 - accuracy: 0.5621 - val_loss: 0.6876 - val_accuracy: 0.5185\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 4s 666ms/step - loss: 0.6923 - accuracy: 0.4837 - val_loss: 0.6874 - val_accuracy: 0.5926\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 2s 431ms/step - loss: 0.6902 - accuracy: 0.6013 - val_loss: 0.6860 - val_accuracy: 0.6296\n",
            "Epoch 7/10\n",
            "5/5 [==============================] - 3s 558ms/step - loss: 0.6911 - accuracy: 0.4706 - val_loss: 0.6839 - val_accuracy: 0.5926\n",
            "Epoch 8/10\n",
            "5/5 [==============================] - 3s 468ms/step - loss: 0.6873 - accuracy: 0.6471 - val_loss: 0.6839 - val_accuracy: 0.5926\n",
            "Epoch 9/10\n",
            "5/5 [==============================] - 3s 448ms/step - loss: 0.6967 - accuracy: 0.4902 - val_loss: 0.6868 - val_accuracy: 0.5185\n",
            "Epoch 10/10\n",
            "5/5 [==============================] - 2s 437ms/step - loss: 0.6883 - accuracy: 0.5425 - val_loss: 0.6813 - val_accuracy: 0.6296\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.6813 - accuracy: 0.6296\n",
            "Test Loss: 0.6813\n",
            "Test Accuracy: 62.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(test_generator)\n",
        "y_test_binary = y_test_split.astype(int)\n",
        "\n",
        "# Thresholds\n",
        "thresholds = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_binary = (y_pred > threshold).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    py_pred_binary = (y_pred > threshold).astype(int)\n",
        "    precision = precision_score(y_test_binary, y_pred_binary)\n",
        "    recall = recall_score(y_test_binary, y_pred_binary)\n",
        "    f1 = f1_score(y_test_binary, y_pred_binary)\n",
        "    logloss = log_loss(y_test_binary, y_pred_binary)\n",
        "    kappa = cohen_kappa_score(y_test_binary, y_pred_binary)\n",
        "\n",
        "    # ROC-AUC requires binary or multilabel indicator format\n",
        "    roc_auc = roc_auc_score(y_test_binary, y_pred_binary)\n",
        "\n",
        "    print(f\"Threshold: {threshold}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Log Loss: {logloss:.4f}, Cohen's Kappa: {kappa:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# Choose an appropriate threshold, let's take 0.5\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "precision = precision_score(y_test_split, y_pred_binary)\n",
        "recall = recall_score(y_test_split, y_pred_binary)\n",
        "f1 = f1_score(y_test_split, y_pred_binary)\n",
        "logloss = log_loss(y_test_split, y_pred_binary)\n",
        "kappa = cohen_kappa_score(y_test_split, y_pred_binary)\n",
        "roc_auc = roc_auc_score(y_test_split, y_pred_binary)\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test_split, y_pred_binary)\n",
        "\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Log Loss:\", logloss)\n",
        "print(\"Cohen's Kappa:\", kappa)\n",
        "print(\"ROC-AUC:\", roc_auc)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__AhJuiRcMmR",
        "outputId": "52a8710a-4896-4f66-9ea5-7e3352c91dd3"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Threshold: 0.2, Precision: 0.4815, Recall: 1.0000, F1: 0.6500, Log Loss: 18.6893, Cohen's Kappa: 0.0000, ROC-AUC: 0.5000\n",
            "Threshold: 0.3, Precision: 0.4815, Recall: 1.0000, F1: 0.6500, Log Loss: 18.6893, Cohen's Kappa: 0.0000, ROC-AUC: 0.5000\n",
            "Threshold: 0.4, Precision: 0.4815, Recall: 1.0000, F1: 0.6500, Log Loss: 18.6893, Cohen's Kappa: 0.0000, ROC-AUC: 0.5000\n",
            "Threshold: 0.5, Precision: 0.4348, Recall: 0.7692, F1: 0.5556, Log Loss: 21.3592, Cohen's Kappa: -0.1551, ROC-AUC: 0.4203\n",
            "Threshold: 0.6, Precision: 0.0000, Recall: 0.0000, F1: 0.0000, Log Loss: 17.3544, Cohen's Kappa: 0.0000, ROC-AUC: 0.5000\n",
            "\n",
            "Overall Metrics:\n",
            "Precision: 0.43478260869565216\n",
            "Recall: 0.7692307692307693\n",
            "F1 Score: 0.5555555555555555\n",
            "Log Loss: 21.35920200836572\n",
            "Cohen's Kappa: -0.15508021390374327\n",
            "ROC-AUC: 0.4203296703296703\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 1 13]\n",
            " [ 3 10]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}